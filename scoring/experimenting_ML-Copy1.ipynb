{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "sys.path.append(path.dirname(path.dirname(path.abspath('__file__'))))\n",
    "\n",
    "import pprint\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import h5py\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/38601026/easy-way-to-use-parallel-options-of-scikit-learn-functions-on-hpc/38814491#38814491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>EH</th>\n",
       "      <th>EA</th>\n",
       "      <th>result_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2997</td>\n",
       "      <td>782</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2073</td>\n",
       "      <td>2072</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7022</td>\n",
       "      <td>6281</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1328</td>\n",
       "      <td>2217</td>\n",
       "      <td>90.143872</td>\n",
       "      <td>109.856128</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4397</td>\n",
       "      <td>3683</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   home_team  away_team          EH          EA  result_final\n",
       "0       2997        782  100.000000  100.000000           1.0\n",
       "1       2073       2072  100.000000  100.000000           1.0\n",
       "2       7022       6281  100.000000  100.000000           1.0\n",
       "3       1328       2217   90.143872  109.856128           2.0\n",
       "4       4397       3683  105.000000   95.000000           0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.read_csv('./final_trans.csv', index_col='Unnamed: 0')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_out_100(data):\n",
    "    print('[Filter]')\n",
    "    dataf = data[(data.EH!=100) & (data.EA!=100)].iloc[:]\n",
    "    return dataf\n",
    "\n",
    "def plot_data_labels(data, labels=['1','X','2'], npoints=1000):\n",
    "    x = data.iloc[:npoints]['EH'].values\n",
    "    y = data.iloc[:npoints]['EA'].values\n",
    "    label = data.iloc[:npoints][result_key].values\n",
    "    colors = ['green','blue','black']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "    plt.xlabel('EH')\n",
    "    plt.ylabel('EA')\n",
    "    cb = plt.colorbar()\n",
    "    loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
    "    cb.set_ticks(loc)\n",
    "    cb.set_ticklabels(labels)\n",
    "    return data\n",
    "\n",
    "def run_pca(data):\n",
    "    print('[PCA]')\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # print(pca.explained_variance_ratio_)  \n",
    "\n",
    "    # print(pca.singular_values_)  \n",
    "\n",
    "    pca.fit(data.values[:,[2,3]])\n",
    "\n",
    "    tx =  pca.transform(data.values[:,[2,3]]) \n",
    "\n",
    "    data.loc[:,['EH','EA']] = tx\n",
    "    return data\n",
    "\n",
    "def classify(data):\n",
    "    print(data.head())\n",
    "    print('[Classifier]')\n",
    "    X = data.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "    # X = final.iloc[:][['EH','EA']]#.values\n",
    "    y = data.iloc[:][result_key]#.values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "    # final.iloc[:50000].tail()\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    print('Accuracy: ',clf.score(X_test, y_test))\n",
    "    # print(metrics.accuracy_score(clf.predict(X_test), y_test))\n",
    "    # clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "\n",
    "    pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "#     predicted = cross_val_predict(clf, X,y, cv=5)\n",
    "    return data\n",
    "\n",
    "def pipeline_func(data, fns):\n",
    "    return reduce(lambda a,x: x(a), fns, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Filter]\n",
      "[PCA]\n"
     ]
    }
   ],
   "source": [
    "fd = run_pca(filter_out_100(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_key = 'result_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = fd.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "y = fd.iloc[:][result_key]#.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "weights = ['distance','uniform']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from distributed.joblib import DistributedBackend \n",
    "\n",
    "# it is important to import joblib from sklearn if we want the distributed features to work with sklearn!\n",
    "from sklearn.externals.joblib import Parallel, parallel_backend, register_parallel_backend\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5,weights=weights[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.parallel import Client\n",
    "ipclient = Client(profile='ssh')\n",
    "\n",
    "e = ipclient.become_dask()\n",
    "\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "register_parallel_backend('distributed', DistributedBackend)\n",
    "\n",
    "%time\n",
    "with parallel_backend('distributed', scheduler_host='192.168.0.101:42434'):\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6406574437745478\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.77      0.66      0.71     20988\\n'\n",
      " '        1.0       0.43      0.50      0.46      7034\\n'\n",
      " '        2.0       0.60      0.70      0.65     11951\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.66      0.64      0.65     39973\\n')\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "X.shape\n",
    "\n",
    "pprint.pprint(metrics.accuracy_score(y_pred, y_test))\n",
    "pprint.pprint(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.6323637023184835\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.67      0.69      0.68     70421\\n'\n",
      " '        1.0       0.49      0.50      0.50     32191\\n'\n",
      " '        2.0       0.67      0.64      0.65     57277\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.63      0.63      0.63    159889\\n')\n",
      "2\n",
      "0.6403317301377831\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.75      0.66      0.70     82330\\n'\n",
      " '        1.0       0.47      0.51      0.49     30491\\n'\n",
      " '        2.0       0.60      0.69      0.64     47068\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.64    159889\\n')\n",
      "3\n",
      "0.6419641126031184\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.72      0.67      0.69     77543\\n'\n",
      " '        1.0       0.46      0.54      0.50     28544\\n'\n",
      " '        2.0       0.65      0.66      0.65     53802\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.64    159889\\n')\n",
      "4\n",
      "0.642552020464197\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.73      0.67      0.70     78558\\n'\n",
      " '        1.0       0.47      0.53      0.50     29160\\n'\n",
      " '        2.0       0.63      0.66      0.65     52171\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.65    159889\\n')\n",
      "5\n",
      "0.6426458355484117\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.72      0.67      0.70     77643\\n'\n",
      " '        1.0       0.47      0.54      0.50     28670\\n'\n",
      " '        2.0       0.64      0.66      0.65     53576\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.65    159889\\n')\n",
      "6\n",
      "0.643239997748438\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.73      0.67      0.70     79349\\n'\n",
      " '        1.0       0.46      0.54      0.50     27759\\n'\n",
      " '        2.0       0.64      0.66      0.65     52781\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.65    159889\\n')\n",
      "7\n",
      "0.642870991750527\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.72      0.67      0.70     77539\\n'\n",
      " '        1.0       0.46      0.55      0.50     27495\\n'\n",
      " '        2.0       0.65      0.65      0.65     54855\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.65    159889\\n')\n",
      "8\n",
      "0.6445096285548099\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.74      0.67      0.70     79408\\n'\n",
      " '        1.0       0.46      0.55      0.50     27349\\n'\n",
      " '        2.0       0.64      0.66      0.65     53132\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.66      0.64      0.65    159889\\n')\n",
      "9\n",
      "0.6447848194685063\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.73      0.67      0.70     79493\\n'\n",
      " '        1.0       0.44      0.56      0.50     26126\\n'\n",
      " '        2.0       0.65      0.65      0.65     54270\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.66      0.64      0.65    159889\\n')\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,10):\n",
    "    print(n)\n",
    "    clf = KNeighborsClassifier(n_neighbors=n,weights=weights[0])\n",
    "    predicted = cross_val_predict(clf, X, y, cv=5)\n",
    "    pprint.pprint(metrics.accuracy_score(predicted, y))\n",
    "    pprint.pprint(metrics.classification_report(predicted, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6419641126031184\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.72      0.67      0.69     77543\\n'\n",
      " '        1.0       0.46      0.54      0.50     28544\\n'\n",
      " '        2.0       0.65      0.66      0.65     53802\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.65      0.64      0.64    159889\\n')\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=3,weights=weights[0])\n",
    "predicted = cross_val_predict(clf, X, y, cv=5)\n",
    "pprint.pprint(metrics.accuracy_score(predicted, y))\n",
    "pprint.pprint(metrics.classification_report(predicted, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAAHjCAYAAABxb1V6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe64af1f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_labels(fd);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "accs=[]\n",
    "clfs = []\n",
    "weights = ['distance','uniform']\n",
    "max_nei = 5\n",
    "for w in weights:\n",
    "    print(w)\n",
    "    acc = []\n",
    "    for n in range(1,max_nei):\n",
    "        print(n)\n",
    "        clf = KNeighborsClassifier(n_neighbors=n,weights=w)\n",
    "        clf.fit(X_train, y_train) \n",
    "        acc.append(clf.score(X_test, y_test))\n",
    "    accs.append(acc)\n",
    "\n",
    "for w, a in enumerate(accs):\n",
    "    plt.plot(range(1,max_nei), a, label=weights[w])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accs=[]\n",
    "clfs = []\n",
    "weights = ['distance','uniform']\n",
    "max_nei = 5\n",
    "for w in weights:\n",
    "    print(w)\n",
    "    acc = []\n",
    "    for n in range(1,max_nei):\n",
    "        print(n)\n",
    "        clf = KNeighborsClassifier(n_neighbors=n,weights=w)\n",
    "        predicted = cross_val_predict(clf, X, y, cv=5)\n",
    "        acc.append(metrics.accuracy_score(predicted , y))\n",
    "\n",
    "    accs.append(acc)\n",
    "\n",
    "for w, a in enumerate(accs):\n",
    "    plt.plot(range(1,max_nei), a, label=weights[w])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = ['distance','uniform']\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5,weights=weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "X.shape\n",
    "\n",
    "pprint.pprint(metrics.accuracy_score(y_pred, y_test))\n",
    "pprint.pprint(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAUSSIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   home_team  away_team         EH         EA  result_final\n",
      "3       1328       2217 -11.205144  -7.983989           2.0\n",
      "4       4397       3683   6.751962   2.922496           0.0\n",
      "5        110       4260  18.322870   9.950240           0.0\n",
      "6       4919       1727  18.322870   9.950240           0.0\n",
      "7       2644       5306  29.240713  16.581336           0.0\n",
      "[Classifier]\n",
      "Accuracy:  0.4824006204187827\n",
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.84      0.48      0.61     31450\\n'\n",
      " '        1.0       0.00      0.00      0.00         0\\n'\n",
      " '        2.0       0.30      0.48      0.37      8523\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.73      0.48      0.56     39973\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasper/PycharmProjects/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "fns = [classify]\n",
    "out = pipeline_func(fd,fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>EH</th>\n",
       "      <th>EA</th>\n",
       "      <th>result_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1328</td>\n",
       "      <td>2217</td>\n",
       "      <td>-11.205144</td>\n",
       "      <td>-7.983989</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4397</td>\n",
       "      <td>3683</td>\n",
       "      <td>6.751962</td>\n",
       "      <td>2.922496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>110</td>\n",
       "      <td>4260</td>\n",
       "      <td>18.322870</td>\n",
       "      <td>9.950240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4919</td>\n",
       "      <td>1727</td>\n",
       "      <td>18.322870</td>\n",
       "      <td>9.950240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2644</td>\n",
       "      <td>5306</td>\n",
       "      <td>29.240713</td>\n",
       "      <td>16.581336</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>837</td>\n",
       "      <td>7516</td>\n",
       "      <td>18.322870</td>\n",
       "      <td>9.950240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5291</td>\n",
       "      <td>4629</td>\n",
       "      <td>18.322870</td>\n",
       "      <td>9.950240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>323</td>\n",
       "      <td>2587</td>\n",
       "      <td>-5.335377</td>\n",
       "      <td>-4.418909</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7804</td>\n",
       "      <td>3431</td>\n",
       "      <td>39.548440</td>\n",
       "      <td>22.841870</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6367</td>\n",
       "      <td>3150</td>\n",
       "      <td>6.751962</td>\n",
       "      <td>2.922496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4320</td>\n",
       "      <td>2379</td>\n",
       "      <td>-5.335377</td>\n",
       "      <td>-4.418909</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4627</td>\n",
       "      <td>1278</td>\n",
       "      <td>12.621729</td>\n",
       "      <td>6.487577</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3621</td>\n",
       "      <td>6495</td>\n",
       "      <td>-11.205144</td>\n",
       "      <td>-7.983989</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5067</td>\n",
       "      <td>5933</td>\n",
       "      <td>6.751962</td>\n",
       "      <td>2.922496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7113</td>\n",
       "      <td>4929</td>\n",
       "      <td>-5.335377</td>\n",
       "      <td>-4.418909</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5047</td>\n",
       "      <td>3618</td>\n",
       "      <td>6.751962</td>\n",
       "      <td>2.922496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1917</td>\n",
       "      <td>6410</td>\n",
       "      <td>6.751962</td>\n",
       "      <td>2.922496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5517</td>\n",
       "      <td>5073</td>\n",
       "      <td>-11.205144</td>\n",
       "      <td>-7.983989</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1727</td>\n",
       "      <td>1328</td>\n",
       "      <td>-11.109392</td>\n",
       "      <td>12.284469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5429</td>\n",
       "      <td>788</td>\n",
       "      <td>18.322870</td>\n",
       "      <td>9.950240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2458</td>\n",
       "      <td>5683</td>\n",
       "      <td>12.621729</td>\n",
       "      <td>6.487577</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1201</td>\n",
       "      <td>7024</td>\n",
       "      <td>12.621729</td>\n",
       "      <td>6.487577</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3683</td>\n",
       "      <td>7113</td>\n",
       "      <td>-2.962410</td>\n",
       "      <td>5.295463</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3618</td>\n",
       "      <td>336</td>\n",
       "      <td>-2.945336</td>\n",
       "      <td>5.365359</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2644</td>\n",
       "      <td>6495</td>\n",
       "      <td>21.300447</td>\n",
       "      <td>-15.924255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7804</td>\n",
       "      <td>5306</td>\n",
       "      <td>37.524844</td>\n",
       "      <td>14.557740</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>837</td>\n",
       "      <td>2587</td>\n",
       "      <td>13.678321</td>\n",
       "      <td>-9.063458</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5291</td>\n",
       "      <td>3150</td>\n",
       "      <td>16.051288</td>\n",
       "      <td>0.650914</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>323</td>\n",
       "      <td>3431</td>\n",
       "      <td>3.476142</td>\n",
       "      <td>31.653389</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5073</td>\n",
       "      <td>1917</td>\n",
       "      <td>9.096419</td>\n",
       "      <td>-7.944219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288842</th>\n",
       "      <td>7758</td>\n",
       "      <td>2921</td>\n",
       "      <td>-7.889825</td>\n",
       "      <td>6.144555</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288847</th>\n",
       "      <td>4619</td>\n",
       "      <td>2900</td>\n",
       "      <td>14.969612</td>\n",
       "      <td>-9.728044</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288848</th>\n",
       "      <td>7656</td>\n",
       "      <td>2609</td>\n",
       "      <td>-14.634703</td>\n",
       "      <td>-2.147327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288850</th>\n",
       "      <td>656</td>\n",
       "      <td>6035</td>\n",
       "      <td>-42.099836</td>\n",
       "      <td>19.854624</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288853</th>\n",
       "      <td>879</td>\n",
       "      <td>937</td>\n",
       "      <td>3.194441</td>\n",
       "      <td>-11.641161</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288854</th>\n",
       "      <td>998</td>\n",
       "      <td>2016</td>\n",
       "      <td>-5.135640</td>\n",
       "      <td>-4.467699</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288856</th>\n",
       "      <td>467</td>\n",
       "      <td>7851</td>\n",
       "      <td>19.249815</td>\n",
       "      <td>9.385311</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288857</th>\n",
       "      <td>7620</td>\n",
       "      <td>2861</td>\n",
       "      <td>-7.645714</td>\n",
       "      <td>6.587485</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288859</th>\n",
       "      <td>4549</td>\n",
       "      <td>3481</td>\n",
       "      <td>7.130073</td>\n",
       "      <td>2.756129</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288861</th>\n",
       "      <td>6926</td>\n",
       "      <td>1275</td>\n",
       "      <td>-5.335377</td>\n",
       "      <td>-4.418909</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288863</th>\n",
       "      <td>6808</td>\n",
       "      <td>1762</td>\n",
       "      <td>13.915667</td>\n",
       "      <td>5.487329</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288871</th>\n",
       "      <td>5031</td>\n",
       "      <td>7158</td>\n",
       "      <td>-10.030965</td>\n",
       "      <td>-3.924346</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288873</th>\n",
       "      <td>4717</td>\n",
       "      <td>4404</td>\n",
       "      <td>3.655217</td>\n",
       "      <td>-30.219226</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288874</th>\n",
       "      <td>5785</td>\n",
       "      <td>7209</td>\n",
       "      <td>6.945054</td>\n",
       "      <td>-7.418697</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288876</th>\n",
       "      <td>4510</td>\n",
       "      <td>5846</td>\n",
       "      <td>21.300447</td>\n",
       "      <td>-15.924255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288877</th>\n",
       "      <td>197</td>\n",
       "      <td>4850</td>\n",
       "      <td>-11.205144</td>\n",
       "      <td>-7.983989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288878</th>\n",
       "      <td>186</td>\n",
       "      <td>3961</td>\n",
       "      <td>16.690308</td>\n",
       "      <td>3.266914</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288879</th>\n",
       "      <td>7697</td>\n",
       "      <td>162</td>\n",
       "      <td>4.333478</td>\n",
       "      <td>-6.780757</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288881</th>\n",
       "      <td>4390</td>\n",
       "      <td>192</td>\n",
       "      <td>-9.280103</td>\n",
       "      <td>10.726038</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288882</th>\n",
       "      <td>6476</td>\n",
       "      <td>2183</td>\n",
       "      <td>22.884098</td>\n",
       "      <td>9.315604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288883</th>\n",
       "      <td>3298</td>\n",
       "      <td>4297</td>\n",
       "      <td>6.988373</td>\n",
       "      <td>-13.372527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288884</th>\n",
       "      <td>6698</td>\n",
       "      <td>1282</td>\n",
       "      <td>3.356146</td>\n",
       "      <td>-11.693022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288885</th>\n",
       "      <td>292</td>\n",
       "      <td>2057</td>\n",
       "      <td>10.115186</td>\n",
       "      <td>18.190678</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288887</th>\n",
       "      <td>710</td>\n",
       "      <td>1670</td>\n",
       "      <td>-7.367277</td>\n",
       "      <td>-12.737033</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288891</th>\n",
       "      <td>5520</td>\n",
       "      <td>7823</td>\n",
       "      <td>13.637355</td>\n",
       "      <td>-9.231162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288894</th>\n",
       "      <td>4422</td>\n",
       "      <td>2515</td>\n",
       "      <td>-16.154014</td>\n",
       "      <td>8.517827</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288895</th>\n",
       "      <td>5496</td>\n",
       "      <td>174</td>\n",
       "      <td>6.751962</td>\n",
       "      <td>2.922496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288896</th>\n",
       "      <td>916</td>\n",
       "      <td>3809</td>\n",
       "      <td>-7.761719</td>\n",
       "      <td>6.112588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288897</th>\n",
       "      <td>5689</td>\n",
       "      <td>373</td>\n",
       "      <td>-4.441933</td>\n",
       "      <td>16.030915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288898</th>\n",
       "      <td>4688</td>\n",
       "      <td>27</td>\n",
       "      <td>-6.487720</td>\n",
       "      <td>-9.136332</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159889 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        home_team  away_team         EH         EA  result_final\n",
       "3            1328       2217 -11.205144  -7.983989           2.0\n",
       "4            4397       3683   6.751962   2.922496           0.0\n",
       "5             110       4260  18.322870   9.950240           0.0\n",
       "6            4919       1727  18.322870   9.950240           0.0\n",
       "7            2644       5306  29.240713  16.581336           0.0\n",
       "8             837       7516  18.322870   9.950240           0.0\n",
       "9            5291       4629  18.322870   9.950240           0.0\n",
       "10            323       2587  -5.335377  -4.418909           2.0\n",
       "12           7804       3431  39.548440  22.841870           0.0\n",
       "13           6367       3150   6.751962   2.922496           0.0\n",
       "15           4320       2379  -5.335377  -4.418909           2.0\n",
       "16           4627       1278  12.621729   6.487577           0.0\n",
       "17           3621       6495 -11.205144  -7.983989           2.0\n",
       "18           5067       5933   6.751962   2.922496           0.0\n",
       "20           7113       4929  -5.335377  -4.418909           2.0\n",
       "21           5047       3618   6.751962   2.922496           0.0\n",
       "22           1917       6410   6.751962   2.922496           0.0\n",
       "24           5517       5073 -11.205144  -7.983989           2.0\n",
       "28           1727       1328 -11.109392  12.284469           0.0\n",
       "30           5429        788  18.322870   9.950240           0.0\n",
       "33           2458       5683  12.621729   6.487577           0.0\n",
       "34           1201       7024  12.621729   6.487577           0.0\n",
       "39           3683       7113  -2.962410   5.295463           2.0\n",
       "40           3618        336  -2.945336   5.365359           0.0\n",
       "41           2644       6495  21.300447 -15.924255           0.0\n",
       "42           7804       5306  37.524844  14.557740           0.0\n",
       "43            837       2587  13.678321  -9.063458           0.0\n",
       "44           5291       3150  16.051288   0.650914           0.0\n",
       "45            323       3431   3.476142  31.653389           1.0\n",
       "47           5073       1917   9.096419  -7.944219           0.0\n",
       "...           ...        ...        ...        ...           ...\n",
       "288842       7758       2921  -7.889825   6.144555           0.0\n",
       "288847       4619       2900  14.969612  -9.728044           1.0\n",
       "288848       7656       2609 -14.634703  -2.147327           0.0\n",
       "288850        656       6035 -42.099836  19.854624           2.0\n",
       "288853        879        937   3.194441 -11.641161           2.0\n",
       "288854        998       2016  -5.135640  -4.467699           2.0\n",
       "288856        467       7851  19.249815   9.385311           2.0\n",
       "288857       7620       2861  -7.645714   6.587485           1.0\n",
       "288859       4549       3481   7.130073   2.756129           1.0\n",
       "288861       6926       1275  -5.335377  -4.418909           1.0\n",
       "288863       6808       1762  13.915667   5.487329           2.0\n",
       "288871       5031       7158 -10.030965  -3.924346           1.0\n",
       "288873       4717       4404   3.655217 -30.219226           0.0\n",
       "288874       5785       7209   6.945054  -7.418697           2.0\n",
       "288876       4510       5846  21.300447 -15.924255           0.0\n",
       "288877        197       4850 -11.205144  -7.983989           0.0\n",
       "288878        186       3961  16.690308   3.266914           0.0\n",
       "288879       7697        162   4.333478  -6.780757           0.0\n",
       "288881       4390        192  -9.280103  10.726038           0.0\n",
       "288882       6476       2183  22.884098   9.315604           0.0\n",
       "288883       3298       4297   6.988373 -13.372527           0.0\n",
       "288884       6698       1282   3.356146 -11.693022           0.0\n",
       "288885        292       2057  10.115186  18.190678           2.0\n",
       "288887        710       1670  -7.367277 -12.737033           2.0\n",
       "288891       5520       7823  13.637355  -9.231162           0.0\n",
       "288894       4422       2515 -16.154014   8.517827           0.0\n",
       "288895       5496        174   6.751962   2.922496           0.0\n",
       "288896        916       3809  -7.761719   6.112588           0.0\n",
       "288897       5689        373  -4.441933  16.030915           0.0\n",
       "288898       4688         27  -6.487720  -9.136332           2.0\n",
       "\n",
       "[159889 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('             precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        0.0       0.50      0.82      0.63     17964\\n'\n",
      " '        1.0       0.57      0.01      0.01      8224\\n'\n",
      " '        2.0       0.51      0.39      0.45     13785\\n'\n",
      " '\\n'\n",
      " 'avg / total       0.52      0.51      0.44     39973\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasper/PycharmProjects/venv/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "predictions2 = clf2.predict(X_test)\n",
    "pprint.pprint(metrics.classification_report(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict={'sklearn.cluster.FeatureAgglomeration': {'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine', 'precomputed'], 'linkage': ['ward', 'complete', 'average']}, 'sklearn.kernel_approximation.RBFSampler': {'gamma': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ...01, 0.1, 1.0, 10.0, 100.0], 'fit_prior': [True, False]}, 'sklearn.preprocessing.StandardScaler': {}},\n",
       "        crossover_rate=0.1, cv=5, disable_update_check=False,\n",
       "        early_stop=None, generations=5, max_eval_time_mins=5,\n",
       "        max_time_mins=None, memory=None, mutation_rate=0.9, n_jobs=1,\n",
       "        offspring_size=13, periodic_checkpoint_folder=None,\n",
       "        population_size=13, random_state=None, scoring=None, subsample=1.0,\n",
       "        verbosity=2, warm_start=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=13, verbosity=2, warm_start=True)\n",
    "tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a07abd874a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/venv/lib/python3.5/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    669\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/venv/lib/python3.5/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/venv/lib/python3.5/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;31m# need raise RuntimeError because no pipeline has been optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A pipeline has not yet been optimized. Please call fit() first.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "clf = tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fns = [filter_out_100,plot_data_labels, classify, run_pca, plot_data_labels, classify]\n",
    "# fns = [filter_out_100,plot_data_labels, run_pca, plot_data_labels]\n",
    "\n",
    "\n",
    "data = filter_out_100(final)\n",
    "\n",
    "clf1 = GaussianNB()\n",
    "\n",
    "X = data.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "# X = final.iloc[:][['EH','EA']]#.values\n",
    "y = data.iloc[:][result_key]#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# final.iloc[:50000].tail()\n",
    "\n",
    "# clf = GaussianNB()\n",
    "clf1 = clf1.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy: ',clf1.score(X_test, y_test))\n",
    "# print(metrics.accuracy_score(clf.predict(X_test), y_test))\n",
    "# clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions1 = clf1.predict(X_test)\n",
    "pprint.pprint(metrics.classification_report(predictions1, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data1 = run_pca(data)\n",
    "\n",
    "final.head()\n",
    "\n",
    "data.head()\n",
    "\n",
    "X = data1.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "# X = final.iloc[:][['EH','EA']]#.values\n",
    "y = data1.iloc[:][result_key]#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# final.iloc[:50000].tail()\n",
    "\n",
    "# clf = GaussianNB()\n",
    "clf2 = clf2.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy: ',clf2.score(X_test, y_test))\n",
    "# print(metrics.accuracy_score(clf.predict(X_test), y_test))\n",
    "# clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions2 = clf2.predict(X_test)\n",
    "pprint.pprint(metrics.classification_report(predictions2, y_test))\n",
    "\n",
    "def choose(p1,p2, key=[0,0]):\n",
    "    if p1==key[0]:\n",
    "        return p1\n",
    "    elif p2==key[1]:\n",
    "        return p2\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "output_df = pd.DataFrame(list(zip(predictions1,predictions2, y_test)), columns=['p1','p2','ytrue'])\n",
    "\n",
    "out1 = []\n",
    "for i, (ix, row) in enumerate(output_df.iterrows()):\n",
    "    out1.append(choose(row.p1,row.p2, key=[0, 0]))\n",
    "\n",
    "out2 = []\n",
    "for i, (ix, row) in enumerate(output_df.iterrows()):\n",
    "    out2.append(choose(row.p1,row.p2,key=[1, 1]))\n",
    "\n",
    "output_df.loc[:, 'fil1'] = out1\n",
    "\n",
    "output_df.loc[:, 'fil2'] = out2\n",
    "\n",
    "fi3 = output_df[['fil1', 'fil2']].sum(axis=1).values.astype(int)\n",
    "\n",
    "output_df.loc[:,'fil3'] = fi3\n",
    "\n",
    "\n",
    "\n",
    "output_df['fil3'].value_counts()\n",
    "output_df[['fil1', 'fil2']].sum(axis=1).value_counts()\n",
    "\n",
    "\n",
    "\n",
    "print(output_df.shape)\n",
    "output_df = output_df.dropna()\n",
    "print(output_df.shape)\n",
    "\n",
    "print(metrics.accuracy_score(output_df['fil3'].values.astype(int), output_df['ytrue'].values.astype(int)))\n",
    "\n",
    "ytrue = output_df['ytrue'].values.astype(int)\n",
    "fi = output_df['fil3'].values.astype(int)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(fi, ytrue))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics.accuracy_score()\n",
    "\n",
    "output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# print(pca.explained_variance_ratio_)  \n",
    "\n",
    "# print(pca.singular_values_)  \n",
    "\n",
    "pca.fit(final2.values[:,[2,3]])\n",
    "\n",
    "tx =  pca.transform(final2.values[:,[2,3]]) \n",
    "\n",
    "final2.loc[:,['EH','EA']] = tx\n",
    "\n",
    "\n",
    "\n",
    "x = final2['EH'].values\n",
    "y = final2['EA'].values\n",
    "label = final2['over_under_2.5'].values\n",
    "colors = ['green','black']\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.xlabel('EH')\n",
    "plt.ylabel('EA')\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(['over','under'])\n",
    "\n",
    "final2.head(2)\n",
    "\n",
    "\n",
    "\n",
    "final2['C_sign'] = np.sign(final2.EH)\n",
    "final2.groupby('over_under_3.5').C_sign.value_counts()\n",
    "\n",
    "### NO PCA\n",
    "\n",
    "final2 = final[(final.EH!=100) | (final.EA!=100)].iloc[:20000]\n",
    "\n",
    "final2.shape\n",
    "\n",
    "\n",
    "X = final2.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "# X = final.iloc[:][['EH','EA']]#.values\n",
    "y = final2.iloc[:]['over_under_2.5']#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# final.iloc[:50000].tail()\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy: ',clf.score(X_test, y_test))\n",
    "# print(metrics.accuracy_score(clf.predict(X_test), y_test))\n",
    "# clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "predicted = cross_val_predict(clf, X,y, cv=5)\n",
    "\n",
    "# metrics.precision_score(y, predicted,pos_label=1) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with pca\n",
    "\n",
    "final2['over_under_2.5'].value_counts(normalize=False).sort_index()\n",
    "\n",
    "final2.shape\n",
    "\n",
    "final2 = final[(final.EH!=100) | (final.EA!=100)].iloc[:20000]\n",
    "\n",
    "pca.fit(final2.values[:,[2,3]])\n",
    "\n",
    "tx =  pca.transform(final2.values[:,[2,3]]) \n",
    "\n",
    "final2.loc[:,['EH','EA']] = tx\n",
    "\n",
    "X = final2.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "# X = final.iloc[:][['EH','EA']]#.values\n",
    "y = final2.iloc[:]['over_under_2.5']#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# final.iloc[:50000].tail()\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy: ',clf.score(X_test, y_test))\n",
    "\n",
    "# clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "X = final2.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "# X = final.iloc[:][['EH','EA']]#.values\n",
    "y = final2.iloc[:]['over_under_2.5']#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# final.iloc[:50000].tail()\n",
    "\n",
    "tpot = TPOTClassifier(generations=1, population_size=13, verbosity=2)\n",
    "\n",
    "clf = tpot.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "X = final2.iloc[:][['home_team','away_team','EH','EA']]#.values\n",
    "# X = final.iloc[:][['EH','EA']]#.values\n",
    "y = final2.iloc[:]['over_under_2.5']#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# final.iloc[:50000].tail()\n",
    "\n",
    "tpot = TPOTClassifier(generations=1, population_size=13, verbosity=2)\n",
    "\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "clf.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %load tpot_mnist_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=42)\n",
    "\n",
    "# Score on the training set was:0.5814399904848592\n",
    "exported_pipeline = KNeighborsClassifier(n_neighbors=69, p=2, weights=\"distance\")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "# X = final.iloc[:50000][['home_team','away_team','EH','EA']]#.values\n",
    "# # X = final.iloc[:][['EH','EA']]#.values\n",
    "# y = final.iloc[:50000]['over_under_2.5']#.values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(X_train, y_train)\n",
    "print(clf1.score(X_test, y_test))\n",
    "predictions1 = clf1.predict(X_test)\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions1, y_test))\n",
    "\n",
    "\n",
    "\n",
    "print(clf2.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %load tpot_mnist_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=42)\n",
    "\n",
    "# Score on the training set was:0.5522662309923989\n",
    "exported_pipeline = GaussianNB()\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = final.iloc[:10000][['home_team','away_team','EH','EA']]#.values\n",
    "y = final.iloc[:10000]['over_under_2.5']#.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Create a simple classifier\n",
    "classifier = svm.LinearSVC(random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_score = classifier.decision_function(X_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score,pos_label=0)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 0.2])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "predictions = tpot.predict_proba(X_test)\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "\n",
    "y_score = clf.decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "aa = pd.DataFrame(list(zip(predictions,y_test)))\n",
    "\n",
    "a = pd.DataFrame(y_train)\n",
    "a['over_under_2.5'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "ys = aa[aa[0]!=0].values\n",
    "\n",
    "yp = ys[:,0]\n",
    "yt = ys[:,1]\n",
    "\n",
    "pprint.pprint(metrics.classification_report(yp, yt))\n",
    "\n",
    "# %load tpot_mnist_pipeline.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'].values, random_state=42)\n",
    "\n",
    "# Score on the training set was:0.5523999202370016\n",
    "exported_pipeline = GaussianNB()\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "\n",
    "X = final.iloc[:][['home_team','away_team','EH','EA']].values\n",
    "y = final.iloc[:]['over_under_2.5'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)\n",
    "\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "predictions = gbm.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "ac = metrics.accuracy_score(predictions, y_test)\n",
    "\n",
    "print(ac)\n",
    "\n",
    "metrics.confusion_matrix(predictions, y_test)\n",
    "\n",
    "\n",
    "conf_arr = metrics.confusion_matrix(predictions, y_test)\n",
    "\n",
    "norm_conf = []\n",
    "for i in conf_arr:\n",
    "    a = 0\n",
    "    tmp_arr = []\n",
    "    a = sum(i, 0)\n",
    "    for j in i:\n",
    "        tmp_arr.append(float(j)/float(a))\n",
    "    norm_conf.append(tmp_arr)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect(1)\n",
    "res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "                interpolation='nearest')\n",
    "\n",
    "width, height = conf_arr.shape\n",
    "\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center')\n",
    "\n",
    "cb = fig.colorbar(res)\n",
    "alphabet1 = ['Predicted-O','Predicted-U']\n",
    "alphabet2 = ['Actual-O','Actual-U']\n",
    "plt.xticks(range(width), alphabet2[:width])\n",
    "plt.yticks(range(height), alphabet1[:height])\n",
    "plt.savefig('confusion_matrix.png', format='png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "digits.data\n",
    "\n",
    "digits.target\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "elohome\n",
    "\n",
    "ar_data[0].astype(int)\n",
    "\n",
    "dfs[1]\n",
    "\n",
    "ar_data = h5f[champ][team][:].T[r]\n",
    "\n",
    "ar_data\n",
    "\n",
    "tot = pd.concat(dfs, axis=0, ignore_index=False)\n",
    "\n",
    "tot\n",
    "\n",
    "h5f.close()\n",
    "\n",
    "len(outcomes)\n",
    "\n",
    "ypred\n",
    "metrics.recall_score(y_pred=ypred, y_true=ytrue, pos_label=0)\n",
    "\n",
    "ypred\n",
    "\n",
    "ytrue\n",
    "transformed_observed_data_elo_ratings.shape\n",
    "\n",
    "bob_says = transformed_observed_data_elo_ratings.reshape(-1,1)[-470:]\n",
    "model = model.fit(bob_says)\n",
    "transformed_observed_data_elo_ratings.reshape(-1, 1)[-10:]\n",
    "\n",
    "bob_says\n",
    "\n",
    "h5f.close()\n",
    "\n",
    "print(\"Bob said:\", \", \".join(map(lambda x: observations[int(x)], bob_says)))\n",
    "print(\"Alice Believes:\", \", \".join(map(lambda x: states[x], alice_hears)))\n",
    "h5f.close()\n",
    "pxx = transition_matrix_pxx(data=df.loc[htd.index]['result_final'].dropna().to_frame(),result='result_final',n_states=3)\n",
    "\n",
    "pyx = emission_probabilities_pyx(x_df=df.loc[htd.index]['result_final'].dropna().to_frame(),x_label='result_final',hidden_states=3,y_df=htd)\n",
    "\n",
    "px = df.loc[htd.index]['result_final'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "px\n",
    "\n",
    "pxx\n",
    "\n",
    "\n",
    "df.loc[htd.index]['result_final'].dropna().to_frame()\n",
    "\n",
    "a = df[((df.home_team==teamh) & (df.away_team==teama)) | ((df.home_team==teama)&(df.away_team==teamh))]\n",
    "\n",
    "a\n",
    "\n",
    "l = pd.concat([ht,at],axis=1)\n",
    "\n",
    "l.plot()\n",
    "# l.loc[a.index].plot()\n",
    "\n",
    "pd.concat([htd,atd],axis=1).plot()\n",
    "\n",
    "pd.concat([ht,at],axis=1).plot()\n",
    "\n",
    "ht.plot()\n",
    "at.plot()\n",
    "\n",
    "# df.loc[995088]\n",
    "\n",
    "# df[((df.home_team==teams[i]) | (df.away_team==teams[i])) & (df.championship==champ)]\n",
    "\n",
    "# df.loc[2054071]\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import trueskill\n",
    "def win_probability(team1, team2):\n",
    "    delta_mu = sum(r.mu for r in team1) - sum(r.mu for r in team2)\n",
    "    sum_sigma = sum(r.sigma ** 2 for r in itertools.chain(team1, team2))\n",
    "    size = len(team1) + len(team2)\n",
    "    denom = math.sqrt(size * (BETA * BETA) + sum_sigma)\n",
    "    ts = trueskill.global_env()\n",
    "    return ts.cdf(delta_mu / denom)\n",
    "\n",
    "BETA = ts.BETA\n",
    "def win_probability(a, b):                                                      \n",
    "    deltaMu = sum([x.mu for x in a]) - sum([x.mu for x in b])                   \n",
    "    sumSigma = sum([x.sigma ** 2 for x in a]) + sum([x.sigma ** 2 for x in b])  \n",
    "    playerCount = len(a) + len(b)                                               \n",
    "    denominator = math.sqrt(playerCount * (BETA * BETA) + sumSigma)             \n",
    "    return cdf(deltaMu / denominator)  \n",
    "\n",
    "teams = np.arange(100)\n",
    "ratings = [Rating() for i in range(100)]\n",
    "for i in range(100):\n",
    "    choice = np.random.choice(teams,size=2,replace=True)\n",
    "    a = ratings[choice[0]]\n",
    "    b = ratings[choice[1]]\n",
    "    q = quality_1vs1(a, b)\n",
    "    wp = win_probability([a],[b])\n",
    "    print(wp)\n",
    "    a, b = ts.rate_1vs1(a, b, drawn=False)\n",
    "    \n",
    "    ratings[choice[0]] = a\n",
    "    ratings[choice[1]] = b\n",
    "\n",
    "mus = [r.mu for r in ratings]\n",
    "sigma = [r.sigma for r in ratings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/5821125/how-to-plot-confusion-matrix-with-string-axis-rather-than-integer-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sudo apt-get install build-essentials gfortran gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
