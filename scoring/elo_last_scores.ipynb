{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "sys.path.append(path.dirname(path.dirname(path.abspath('__file__'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scoring.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasper/PycharmProjects/venv/lib/python3.5/site-packages/numpy/lib/arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/kasper/Dropbox/Scrapping/soccerway/csv/final_data_soccerway.csv', index_col='Unnamed: 0')\n",
    "pred_data = pd.read_csv('../data/predict.csv', index_col='Unnamed: 0')\n",
    "df_teams = pred_data[['home_team','away_team']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_key = 'result_final'\n",
    "n_states = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_teams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:00] |#                                  | (ETA:      N/A) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:02] |#                                  | (Time: 0:00:02) \n"
     ]
    }
   ],
   "source": [
    "bar = progressbar.ProgressBar(widgets=[\n",
    "    ' [', progressbar.Timer(), '] ',\n",
    "    progressbar.Bar(),\n",
    "    ' (', progressbar.ETA(), ') ',\n",
    "])\n",
    "\n",
    "ps = []\n",
    "try:\n",
    "    n_start=int(sys.argv[1])\n",
    "    n_end=int(sys.argv[2])\n",
    "except Exception as e:\n",
    "#     print(e)\n",
    "    n_start=10\n",
    "    n_end=df_teams.shape[0]\n",
    "print(n_start,n_end)\n",
    "last_scores =  []\n",
    "ps = []\n",
    "for p, pair in bar(enumerate(df_teams[n_start:n_end, :],start=n_start)):\n",
    "    champ, teams, teamh, teama, datah, dataa = get_team_data(p, pair)\n",
    "    ht, at, htd, atd, last_score_home, last_score_away = get_ratings(datah, dataa, teamh, teama)\n",
    "    last_scores.append([last_score_home, last_score_away])\n",
    "    ps.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = np.array(last_scores)\n",
    "last_scores_df = pd.DataFrame(ls,columns=['EH','EA'], index=ps)\n",
    "last_scores_df.head()\n",
    "last_scores_df.to_csv('./elo_last.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ###### generate dataset for classifier\n",
    "\n",
    "# for p, pair in enumerate(df_teams[:2]):\n",
    "#     print(pair)\n",
    "# #     p = 0\n",
    "\n",
    "#     h5f = h5py.File('./elo_temp/elo_pairs_'+str(p)+'.h5','r')\n",
    "\n",
    "#     champ = list(h5f.keys())[0]\n",
    "\n",
    "#     teams = list(h5f[champ].keys())\n",
    "\n",
    "#     ixs = np.array([])\n",
    "#     for team in teams[:]:    \n",
    "#         ix = h5f[champ][team][:].T[:,0]\n",
    "#         ixs = np.append(ixs, ix)\n",
    "#     uni = np.unique(ixs).astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "#     da = df.loc[uni][['home_team','away_team','result_final','over_under_0.5','over_under_1.5','over_under_2.5','over_under_3.5','over_under_4.5']]\n",
    "\n",
    "\n",
    "#     elos =np.zeros((da.shape[0],2))\n",
    "#     for i, (ix, row) in enumerate(da.iloc[:].iterrows()):\n",
    "#         if i%1000==0:\n",
    "#             print(i,da.shape[0])\n",
    "#         try:\n",
    "#             hdhome = h5f[champ][row.home_team][:].T\n",
    "#             hdaway = h5f[champ][row.away_team][:].T\n",
    "#             elohome = hdhome[np.where(hdhome==ix)[1],1]\n",
    "#             eloaway = hdaway[np.where(hdaway==ix)[1],1]\n",
    "#             elos[i,0] = elohome\n",
    "#             elos[i,1] = eloaway\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     elos_df = pd.DataFrame(elos, columns=['EH','EA'], index=da.index)\n",
    "\n",
    "#     final = pd.concat([da,elos_df],axis=1)\n",
    "# #     final = final.dropna()\n",
    "\n",
    "#     final.to_csv('./elo_goals_data_for_classifier/data_'+str(p)+'.csv')\n",
    "#     h5f.close()\n",
    "\n",
    "# final.head()\n",
    "\n",
    "# final = pd.read_csv('./data_'+str(p)+'.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# data = final.iloc[:]\n",
    "# data.head(5)\n",
    "\n",
    "# ht = final.home_team.values\n",
    "# at = final.away_team.values\n",
    "# all_teams = np.append(ht,at)\n",
    "# un = np.unique(all_teams)\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(un)\n",
    "\n",
    "# # transformed_observed_data_elo_ratings = le.transform(ys)\n",
    "\n",
    "# final.columns\n",
    "\n",
    "# final.loc[:,'home_team'] = le.transform(final.home_team.values)\n",
    "# final.loc[:,'away_team'] =  le.transform(final.away_team.values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X = final.iloc[:10000][['home_team','away_team','EH','EA']].values\n",
    "# y = final.iloc[:10000]['over_under_2.5'].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "# tpot = TPOTClassifier(generations=5, population_size=13, verbosity=2)\n",
    "\n",
    "# tpot.fit(X_train, y_train)\n",
    "\n",
    "# print(tpot.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tpot.export('tpot_mnist_pipeline.py')\n",
    "\n",
    "# # %load tpot_mnist_pipeline.py\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# from sklearn.kernel_approximation import RBFSampler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.pipeline import make_pipeline, make_union\n",
    "# from tpot.builtins import StackingEstimator\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "# features = tpot_data.drop('target', axis=1).values\n",
    "# training_features, testing_features, training_target, testing_target = \\\n",
    "#             train_test_split(features, tpot_data['target'].values, random_state=42)\n",
    "\n",
    "# # Score on the training set was:0.623731933095674\n",
    "# exported_pipeline = make_pipeline(\n",
    "#     VarianceThreshold(threshold=0.9000000000000001),\n",
    "#     StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.1, max_depth=8, max_features=0.05, min_samples_leaf=4, min_samples_split=15, n_estimators=100, subsample=0.8500000000000001)),\n",
    "#     RBFSampler(gamma=0.1),\n",
    "#     XGBClassifier(learning_rate=0.01, max_depth=6, min_child_weight=13, n_estimators=100, nthread=1, subsample=0.05)\n",
    "# )\n",
    "\n",
    "# exported_pipeline.fit(training_features, training_target)\n",
    "# results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "\n",
    "# X = final.iloc[:][['home_team','away_team','EH','EA']].values\n",
    "# y = final.iloc[:]['over_under_2.5'].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)\n",
    "\n",
    "# gbm.fit(X_train, y_train)\n",
    "# predictions = gbm.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# pprint.pprint(metrics.classification_report(predictions, y_test))\n",
    "\n",
    "# ac = metrics.accuracy_score(predictions, y_test)\n",
    "\n",
    "# print(ac)\n",
    "\n",
    "# metrics.confusion_matrix(predictions, y_test)\n",
    "\n",
    "\n",
    "# conf_arr = metrics.confusion_matrix(predictions, y_test)\n",
    "\n",
    "# norm_conf = []\n",
    "# for i in conf_arr:\n",
    "#     a = 0\n",
    "#     tmp_arr = []\n",
    "#     a = sum(i, 0)\n",
    "#     for j in i:\n",
    "#         tmp_arr.append(float(j)/float(a))\n",
    "#     norm_conf.append(tmp_arr)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.clf()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.set_aspect(1)\n",
    "# res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "#                 interpolation='nearest')\n",
    "\n",
    "# width, height = conf_arr.shape\n",
    "\n",
    "# for x in range(width):\n",
    "#     for y in range(height):\n",
    "#         ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
    "#                     horizontalalignment='center',\n",
    "#                     verticalalignment='center')\n",
    "\n",
    "# cb = fig.colorbar(res)\n",
    "# alphabet1 = ['Predicted-O','Predicted-U']\n",
    "# alphabet2 = ['Actual-O','Actual-U']\n",
    "# plt.xticks(range(width), alphabet2[:width])\n",
    "# plt.yticks(range(height), alphabet1[:height])\n",
    "# plt.savefig('confusion_matrix.png', format='png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from tpot import TPOTClassifier\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# digits = load_digits()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "#                                                     train_size=0.75, test_size=0.25)\n",
    "\n",
    "# digits.data\n",
    "\n",
    "# digits.target\n",
    "\n",
    "# tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2)\n",
    "# tpot.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# elohome\n",
    "\n",
    "# ar_data[0].astype(int)\n",
    "\n",
    "# dfs[1]\n",
    "\n",
    "# ar_data = h5f[champ][team][:].T[r]\n",
    "\n",
    "# ar_data\n",
    "\n",
    "# tot = pd.concat(dfs, axis=0, ignore_index=False)\n",
    "\n",
    "# tot\n",
    "\n",
    "# h5f.close()\n",
    "\n",
    "# len(outcomes)\n",
    "\n",
    "# ypred\n",
    "# metrics.recall_score(y_pred=ypred, y_true=ytrue, pos_label=0)\n",
    "\n",
    "# ypred\n",
    "\n",
    "# ytrue\n",
    "# transformed_observed_data_elo_ratings.shape\n",
    "\n",
    "# bob_says = transformed_observed_data_elo_ratings.reshape(-1,1)[-470:]\n",
    "# model = model.fit(bob_says)\n",
    "# transformed_observed_data_elo_ratings.reshape(-1, 1)[-10:]\n",
    "\n",
    "# bob_says\n",
    "\n",
    "# h5f.close()\n",
    "\n",
    "# print(\"Bob said:\", \", \".join(map(lambda x: observations[int(x)], bob_says)))\n",
    "# print(\"Alice Believes:\", \", \".join(map(lambda x: states[x], alice_hears)))\n",
    "# h5f.close()\n",
    "# pxx = transition_matrix_pxx(data=df.loc[htd.index]['result_final'].dropna().to_frame(),result='result_final',n_states=3)\n",
    "\n",
    "# pyx = emission_probabilities_pyx(x_df=df.loc[htd.index]['result_final'].dropna().to_frame(),x_label='result_final',hidden_states=3,y_df=htd)\n",
    "\n",
    "# px = df.loc[htd.index]['result_final'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# px\n",
    "\n",
    "# pxx\n",
    "\n",
    "\n",
    "# df.loc[htd.index]['result_final'].dropna().to_frame()\n",
    "\n",
    "# a = df[((df.home_team==teamh) & (df.away_team==teama)) | ((df.home_team==teama)&(df.away_team==teamh))]\n",
    "\n",
    "# a\n",
    "\n",
    "# l = pd.concat([ht,at],axis=1)\n",
    "\n",
    "# l.plot()\n",
    "# # l.loc[a.index].plot()\n",
    "\n",
    "# pd.concat([htd,atd],axis=1).plot()\n",
    "\n",
    "# pd.concat([ht,at],axis=1).plot()\n",
    "\n",
    "# ht.plot()\n",
    "# at.plot()\n",
    "\n",
    "# # df.loc[995088]\n",
    "\n",
    "# # df[((df.home_team==teams[i]) | (df.away_team==teams[i])) & (df.championship==champ)]\n",
    "\n",
    "# # df.loc[2054071]\n",
    "\n",
    "# import itertools\n",
    "# import math\n",
    "# import trueskill\n",
    "# def win_probability(team1, team2):\n",
    "#     delta_mu = sum(r.mu for r in team1) - sum(r.mu for r in team2)\n",
    "#     sum_sigma = sum(r.sigma ** 2 for r in itertools.chain(team1, team2))\n",
    "#     size = len(team1) + len(team2)\n",
    "#     denom = math.sqrt(size * (BETA * BETA) + sum_sigma)\n",
    "#     ts = trueskill.global_env()\n",
    "#     return ts.cdf(delta_mu / denom)\n",
    "\n",
    "# BETA = ts.BETA\n",
    "# def win_probability(a, b):                                                      \n",
    "#     deltaMu = sum([x.mu for x in a]) - sum([x.mu for x in b])                   \n",
    "#     sumSigma = sum([x.sigma ** 2 for x in a]) + sum([x.sigma ** 2 for x in b])  \n",
    "#     playerCount = len(a) + len(b)                                               \n",
    "#     denominator = math.sqrt(playerCount * (BETA * BETA) + sumSigma)             \n",
    "#     return cdf(deltaMu / denominator)  \n",
    "\n",
    "# teams = np.arange(100)\n",
    "# ratings = [Rating() for i in range(100)]\n",
    "# for i in range(100):\n",
    "#     choice = np.random.choice(teams,size=2,replace=True)\n",
    "#     a = ratings[choice[0]]\n",
    "#     b = ratings[choice[1]]\n",
    "#     q = quality_1vs1(a, b)\n",
    "#     wp = win_probability([a],[b])\n",
    "#     print(wp)\n",
    "#     a, b = ts.rate_1vs1(a, b, drawn=False)\n",
    "    \n",
    "#     ratings[choice[0]] = a\n",
    "#     ratings[choice[1]] = b\n",
    "\n",
    "# mus = [r.mu for r in ratings]\n",
    "# sigma = [r.sigma for r in ratings]\n",
    "\n",
    "# ### sources\n",
    "\n",
    "# #https://stackoverflow.com/questions/5821125/how-to-plot-confusion-matrix-with-string-axis-rather-than-integer-in-python\n",
    "\n",
    "# # https://stackoverflow.com/questions/5375624/a-decorator-that-profiles-a-method-call-and-logs-the-profiling-result\n",
    "\n",
    "# def profileit(prof_fname, sort_field='cumtime'):\n",
    "#     \"\"\"\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     prof_fname\n",
    "#         profile output file name\n",
    "#     sort_field\n",
    "#         \"calls\"     : (((1,-1),              ), \"call count\"),\n",
    "#         \"ncalls\"    : (((1,-1),              ), \"call count\"),\n",
    "#         \"cumtime\"   : (((3,-1),              ), \"cumulative time\"),\n",
    "#         \"cumulative\": (((3,-1),              ), \"cumulative time\"),\n",
    "#         \"file\"      : (((4, 1),              ), \"file name\"),\n",
    "#         \"filename\"  : (((4, 1),              ), \"file name\"),\n",
    "#         \"line\"      : (((5, 1),              ), \"line number\"),\n",
    "#         \"module\"    : (((4, 1),              ), \"file name\"),\n",
    "#         \"name\"      : (((6, 1),              ), \"function name\"),\n",
    "#         \"nfl\"       : (((6, 1),(4, 1),(5, 1),), \"name/file/line\"),\n",
    "#         \"pcalls\"    : (((0,-1),              ), \"primitive call count\"),\n",
    "#         \"stdname\"   : (((7, 1),              ), \"standard name\"),\n",
    "#         \"time\"      : (((2,-1),              ), \"internal time\"),\n",
    "#         \"tottime\"   : (((2,-1),              ), \"internal time\"),\n",
    "#     Returns\n",
    "#     -------\n",
    "#     None\n",
    "\n",
    "#     \"\"\"\n",
    "#     def actual_profileit(func):\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             prof = cProfile.Profile()\n",
    "#             retval = prof.runcall(func, *args, **kwargs)\n",
    "#             stat_fname = '{}.stat'.format(prof_fname)\n",
    "#             prof.dump_stats(prof_fname)\n",
    "#             print_profiler(prof_fname, stat_fname, sort_field)\n",
    "#             print('dump stat in {}'.format(stat_fname))\n",
    "#             return retval\n",
    "#         return wrapper\n",
    "#     return actual_profileit\n",
    "\n",
    "\n",
    "# def print_profiler(profile_input_fname, profile_output_fname, sort_field='cumtime'):\n",
    "#     import pstats\n",
    "#     with open(profile_output_fname, 'w') as f:\n",
    "#         stats = pstats.Stats(profile_input_fname, stream=f)\n",
    "#         stats.sort_stats(sort_field)\n",
    "#         stats.print_stats()\n",
    "\n",
    "#         import cProfile\n",
    "# import functools\n",
    "\n",
    "# def profileit(func):\n",
    "#     @functools.wraps(func)  # <-- Changes here.\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         datafn = func.__name__ + \".profile\" # Name the data file sensibly\n",
    "#         prof = cProfile.Profile()\n",
    "#         retval = prof.runcall(func, *args, **kwargs)\n",
    "#         prof.dump_stats(datafn)\n",
    "#         print(prof.__str__)\n",
    "#         return retval\n",
    "\n",
    "#     return wrapper\n",
    "\n",
    "# @profilei\n",
    "\n",
    "\n",
    "\n",
    "# test()\n",
    "\n",
    "# en1 = cluster[0]\n",
    "\n",
    "# async_results = []\n",
    "\n",
    "# a = view.apply_async(get_ratings,datah, dataa, teamh, teama)\n",
    "\n",
    "\n",
    "# async_results.append(a)\n",
    "# # cluster.wait(async_results)\n",
    "# cluster.wait_interactive(async_results) \n",
    "\n",
    "# async_results[0].error\n",
    "\n",
    "# # %%timeit\n",
    "\n",
    "# for p, pair in enumerate(df_teams[:10]):\n",
    "#     something_1(p,pair)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # %time\n",
    "# async_results = []\n",
    "# for p, pair in enumerate(df_teams[:10]):\n",
    "#     print(pair)\n",
    "#     ar = view.apply_async(something_1, p, pair)\n",
    "#     async_results.append(ar)\n",
    "# # cluster.wait(async_results)\n",
    "# cluster.wait_interactive(async_results) \n",
    "\n",
    "# r = async_results[0]\n",
    "\n",
    "# r.done()\n",
    "\n",
    "# r.error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
